{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Model Training.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1ysy4TNqFPhF89ZulVE3Jul4STbVeo5NY",
      "authorship_tag": "ABX9TyPCWfBR8TeuSIk3qeQg7itP"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6ypsGh-xTJZ",
        "colab_type": "text"
      },
      "source": [
        "# Named Entity Recognition - Model Training\n",
        "\n",
        "If you are new to this, I suggest you to read [Data Preprocessing File](https://github.com/akash1309/Named-Entity-Recognition/blob/master/Data_Preprocessing.ipynb)\n",
        "\n",
        "\n",
        "Tags of entities are encoded in a BIO-annotation scheme. Each entity is labeled with a B or an I to detect multi-word entities, where B denotes the beginning of an entity and I denote the inside of an entity.\n",
        "O denotes all other words which are not named entities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sqaBq7zzKlav",
        "colab_type": "text"
      },
      "source": [
        "## 1) Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rbp5bOM9xLPP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.functional as F\n",
        "import spacy\n",
        "import nltk\n",
        "import os\n",
        "import json\n",
        "import warnings\n",
        "import seaborn\n",
        "import keras\n",
        "from torch.utils.data import DataLoader,TensorDataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPeMJqaW--zK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyper parameters for the vocab\n",
        "\n",
        "PAD_WORD = '<pad>'\n",
        "PAD_TAG = '0'\n",
        "UNK_WORD = 'UNK'\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkXeJ1PdLNCq",
        "colab_type": "text"
      },
      "source": [
        "## 2) Loading the text data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB-kKnHdLQ0u",
        "colab_type": "text"
      },
      "source": [
        "- In NLP, we have `text` as input and our machine can't understand texts. So, our first step is to make a dictionary which stores a `numerical value` corresponding the a `word`.\n",
        "\n",
        "- In NLP applications, a sentence is represented by the sequence of indices of the words in the sentence. \n",
        "      For example if our vocabulary is {'is':1, 'John':2, 'Where':3, '.':4, '?':5} \n",
        "      then the sentence “Where is John ?” is represented as [3,1,2,5]. \n",
        "\n",
        "- We read the words.txt file and populate our vocabulary:\n",
        "\n",
        "We will be working with full datasets, if you want u can work on small as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VvFnuuDJKv9o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for words.txt\n",
        "\n",
        "word_filepath = 'drive/My Drive/Pytorch_DataSet/Named Entity Recognition/big/words.txt'\n",
        "word_to_idx = {}\n",
        "with open(word_filepath,'r') as f:\n",
        "\n",
        "  for i,word in enumerate(set(f.read().splitlines())):\n",
        "    word_to_idx[word] = i+2   # Because first 2 indices are stored for padding and unknown character\n",
        "\n",
        "word_to_idx['<pad>'] = 0  # padding\n",
        "word_to_idx['UNK'] = 1    # unknown\n",
        "\n",
        "idx_to_word = {index: word for word, index in word_to_idx.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1iw1VySoMau3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "d207cee8-e0f6-4a3d-819f-11784c979f4d"
      },
      "source": [
        "print(word_to_idx['<pad>'])\n",
        "print(word_to_idx['UNK'])\n",
        "#print(idx_to_word)\n",
        "print(len(word_to_idx))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n",
            "35180\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1s1DQqiM-Xh",
        "colab_type": "text"
      },
      "source": [
        "In a similar way, we load a mapping `tag_map` from our `labels` from `tags.txt` to indices. Doing so gives us indices for labels in the range `[0,1,...,NUM_TAGS-1]`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPhlpZ8zMcvN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for tags.txt\n",
        "\n",
        "tags_filepath = 'drive/My Drive/Pytorch_DataSet/Named Entity Recognition/big/tags.txt'\n",
        "tag_to_idx = {}\n",
        "\n",
        "with open(tags_filepath,'r') as f:\n",
        "\n",
        "  for i,word in enumerate(set(f.read().splitlines())):\n",
        "    tag_to_idx[word] = i+1 # Because we are storing 0th index for padding \n",
        "tag_to_idx['<pad>'] = 0 # padding\n",
        "idx_to_tag = {index: word for word, index in tag_to_idx.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMiy2FjsOaJ1",
        "colab_type": "code",
        "outputId": "8e321f99-2db8-4416-b98c-9fa02ea1825b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "print(tag_to_idx['<pad>'])\n",
        "print(idx_to_tag[1])\n",
        "print(len(tag_to_idx))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "B-per\n",
            "18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQeeHkFBOulb",
        "colab_type": "text"
      },
      "source": [
        "In addition to words read from English sentences, `words.txt` contains two special tokens: an `UNK` token to represent any word that is not present in the vocabulary, and a `PAD` token that is used as a filler token at the end of a sentence when one batch has sentences of unequal lengths."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61n0zQtDO7ED",
        "colab_type": "text"
      },
      "source": [
        "We are now ready to load our data. We read the sentences in our dataset and convert them to a sequence of indices by looking up the vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTPRjHWUSeJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function for sentences.txt file \n",
        "\n",
        "def encode_sentences(file_path):\n",
        "\n",
        "  sentences = []\n",
        "  \n",
        "  with open(file_path) as f:\n",
        "    for sent in f.read().splitlines():\n",
        "      #replace each token by its index if it is in vocab else use index of UNK\n",
        "      s = []\n",
        "      for token in sent.split(' '):\n",
        "        if token in word_to_idx:\n",
        "          s.append(word_to_idx[token])\n",
        "        else:\n",
        "          s.append(word_to_idx['UNK'])  \n",
        "\n",
        "      sentences.append(s)\n",
        "\n",
        "  return sentences    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vamBvnjoTljc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function for labels.txt file \n",
        "\n",
        "def encode_labels(file_path):\n",
        "\n",
        "  labels = []\n",
        "  with open(file_path) as f:\n",
        "    for sentence in f.read().splitlines():\n",
        "      l = [tag_to_idx[label] for label in sentence.split(' ')]\n",
        "      labels.append(l)\n",
        "\n",
        "  return labels   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OP72mIAOe1m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "16ed6db6-ce75-47ab-a568-b5c3a42dba7d"
      },
      "source": [
        "# Lets apply the transformation\n",
        "\n",
        "file_sentences = 'drive/My Drive/Pytorch_DataSet/Named Entity Recognition/big/sentences.txt'\n",
        "file_labels = 'drive/My Drive/Pytorch_DataSet/Named Entity Recognition/big/labels.txt'\n",
        "\n",
        "sentences = encode_sentences(file_sentences)\n",
        "labels = encode_labels(file_labels)\n",
        "\n",
        "print(len(sentences))\n",
        "print(\"-------------\")\n",
        "print(len(labels))\n",
        "# print(sentences[0])\n",
        "# print(labels[0])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "47959\n",
            "-------------\n",
            "47959\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnGcglhKZqn-",
        "colab_type": "text"
      },
      "source": [
        "## 3) Padding Sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yoggq3oAZ5WS",
        "colab_type": "text"
      },
      "source": [
        "- This is where it gets fun. When we sample a batch of sentences, not all the sentences usually have the same length. Let’s say we have a batch of sentences `batch_sentences` that is a Python list of lists, with its corresponding `batch_tags` which has a tag for each token in `batch_sentences`. \n",
        "\n",
        "- We add pad sequences at last in sentences. Here, we will be taking max length sentence as our main sentence and then padd `<pad>` at the end of all the sentences so that all sequences have all lengths. Similarly in the labels also, we add `O` at last of every label so that all lengths become the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UR8iJnhWc-LB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_sentences = sentences.copy()\n",
        "batch_tags = labels.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6U6MLGtZSRpn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#compute length of longest sentence in batch\n",
        "batch_max_len = max([len(s) for s in batch_sentences])\n",
        "\n",
        "#prepare a numpy array with the data, initializing the data with 'PAD' \n",
        "#and all labels with -1; initializing word_to_idx labels to -1 differentiates tokens \n",
        "#with tags from 'PAD' tokens\n",
        "\n",
        "batch_data = word_to_idx['<pad>']*np.ones((len(batch_sentences), batch_max_len))\n",
        "batch_labels = tag_to_idx['<pad>']*np.ones((len(batch_sentences), batch_max_len))\n",
        "\n",
        "#copy the data to the numpy array\n",
        "for j in range(len(batch_sentences)):\n",
        "  cur_len = len(batch_sentences[j])\n",
        "  batch_data[j][:cur_len] = batch_sentences[j]\n",
        "  batch_labels[j][:cur_len] = batch_tags[j]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3bo_FGTdRuK",
        "colab_type": "code",
        "outputId": "ce22b0fe-2309-4693-d08a-e7d0f80ac6aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# print(batch_data[0])\n",
        "print(len(batch_data[0]))\n",
        "print(type(batch_data))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "104\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM21zuztm4zE",
        "colab_type": "code",
        "outputId": "b286cab6-717b-4e40-cf55-e5ae048bc5a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# print(batch_labels[0])\n",
        "print(len(batch_labels[0]))\n",
        "print(type(batch_data))"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "104\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bkg_OJTMQog",
        "colab_type": "text"
      },
      "source": [
        "## 4) One hot encoding on labels\n",
        "\n",
        "https://stackoverflow.com/questions/29831489/convert-array-of-indices-to-1-hot-encoded-numpy-array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTMu9APfMynv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "9899fba0-9337-485b-f506-e47d45fd5622"
      },
      "source": [
        "a = np.array([1, 0, 3])\n",
        "b = np.zeros((a.size, a.max()+1))\n",
        "b[np.arange(a.size),a] = 1\n",
        "print(b)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 1. 0. 0.]\n",
            " [1. 0. 0. 0.]\n",
            " [0. 0. 0. 1.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SDn7ICUXm9Et",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = batch_labels.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2bCEMQ7Mdmu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bff942b9-f51d-4f07-9998-67d834ddfc40"
      },
      "source": [
        "\"\"\"\n",
        "for a in labels:\n",
        "  b = np.zeros(a.size,a.max() + 1)\n",
        "  b[np.arrange(a.size),a] = 1\n",
        "  print(b)\n",
        "  break\n",
        "\"\"\"  "
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nfor a in labels:\\n  b = np.zeros(a.size,a.max() + 1)\\n  b[np.arrange(a.size),a] = 1\\n  print(b)\\n  break\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwyE7xL0PyrH",
        "colab_type": "text"
      },
      "source": [
        "Here we can see that lists don't have punctuation marks in between them, so when we try to perform one hot encoding, it will always give `TypeError: data type not understood`\n",
        "Uncomment the above code to see the error.\n",
        "\n",
        "Keras has one functionality that can convert it, lets try that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kKfQb6UQ3vA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ef9b5573-7ca2-44d0-e536-7cec8b2c18be"
      },
      "source": [
        "n_tags = len(idx_to_tag)\n",
        "n_tags"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "18"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIMF0yX9NQ8y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.utils import to_categorical\n",
        "# One-Hot encode\n",
        "n_tags = len(labels[0])\n",
        "y = [to_categorical(i, num_classes=n_tags) for i in labels]  # n_tags = total tags + <PAD>\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Am-VmO0URDzH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "5302d663-5f11-4c27-b2e6-1c2bcfa9ab47"
      },
      "source": [
        "y[0]"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLGJHK-jRvpG",
        "colab_type": "text"
      },
      "source": [
        "## 5) Splitting Using TensorDataset and DataLoader\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV8gXyhkRFlY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "30f35381-0e51-4def-bba8-2d37bd12f1a2"
      },
      "source": [
        "len(sentences)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "47959"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjhm1H2YWwtF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = torch.LongTensor(batch_data)\n",
        "y = torch.LongTensor(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lagjhvywR5mf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = TensorDataset(X,y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K16bjUqiWRh7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "27bdc24b-d17b-4d53-e463-d7675126e642"
      },
      "source": [
        "data"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataset.TensorDataset at 0x7fd588ef90f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQJteqmzXKgb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = DataLoader(data,batch_size=32,shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "067tMfbzXsPe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7cc856bd-2cd2-43e8-f18f-bc6203cc477c"
      },
      "source": [
        "dataset"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7fd588ef92b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl2e74sQXt3w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 986
        },
        "outputId": "c0ec9396-75a3-4e74-c3b7-6555ad78f272"
      },
      "source": [
        "for batch,sample in enumerate(dataset):\n",
        "  print(batch)\n",
        "  print(\"<---------->\")\n",
        "  print(sample)\n",
        "  break"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "<---------->\n",
            "[tensor([[32729, 34818, 14341,  ...,     0,     0,     0],\n",
            "        [28198,  7538, 29931,  ...,     0,     0,     0],\n",
            "        [31485,  7516,  8059,  ...,     0,     0,     0],\n",
            "        ...,\n",
            "        [30433, 17858,  7516,  ...,     0,     0,     0],\n",
            "        [30433, 25062,  9279,  ...,     0,     0,     0],\n",
            "        [ 2330,  2417, 17471,  ...,     0,     0,     0]]), tensor([[[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [1, 0, 0,  ..., 0, 0, 0],\n",
            "         [1, 0, 0,  ..., 0, 0, 0],\n",
            "         [1, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [1, 0, 0,  ..., 0, 0, 0],\n",
            "         [1, 0, 0,  ..., 0, 0, 0],\n",
            "         [1, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [1, 0, 0,  ..., 0, 0, 0],\n",
            "         [1, 0, 0,  ..., 0, 0, 0],\n",
            "         [1, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        ...,\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [1, 0, 0,  ..., 0, 0, 0],\n",
            "         [1, 0, 0,  ..., 0, 0, 0],\n",
            "         [1, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [1, 0, 0,  ..., 0, 0, 0],\n",
            "         [1, 0, 0,  ..., 0, 0, 0],\n",
            "         [1, 0, 0,  ..., 0, 0, 0]],\n",
            "\n",
            "        [[0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         [0, 0, 0,  ..., 0, 0, 0],\n",
            "         ...,\n",
            "         [1, 0, 0,  ..., 0, 0, 0],\n",
            "         [1, 0, 0,  ..., 0, 0, 0],\n",
            "         [1, 0, 0,  ..., 0, 0, 0]]])]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS5qmPfAXv8l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}